{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e11df73c",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2024-10-21T12:16:01.420924Z",
          "iopub.status.busy": "2024-10-21T12:16:01.420351Z",
          "iopub.status.idle": "2024-10-21T12:39:06.577559Z",
          "shell.execute_reply": "2024-10-21T12:39:06.576332Z"
        },
        "papermill": {
          "duration": 1385.166964,
          "end_time": "2024-10-21T12:39:06.580530",
          "exception": false,
          "start_time": "2024-10-21T12:16:01.413566",
          "status": "completed"
        },
        "tags": [],
        "id": "e11df73c"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "import requests\n",
        "import os\n",
        "import time\n",
        "\n",
        "# OpenAI API Key\n",
        "api_key = \"*********\"\n",
        "\n",
        "# Function to encode the image\n",
        "def encode_image(image_path):\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
        "\n",
        "# Path to your image\n",
        "input_dir = \"/kaggle/input/ppe-detection-gpt-version/PPE_Detection 2.v1i.yolov8-obb/test/images\"\n",
        "output_dir = \"/kaggle/working/test_set\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "for file_name in os.listdir(input_dir):\n",
        "    filepath = os.path.join(input_dir, file_name)\n",
        "    base64_image = encode_image(filepath)\n",
        "\n",
        "    headers = {\n",
        "      \"Content-Type\": \"application/json\",\n",
        "      \"Authorization\": f\"Bearer {api_key}\"\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "      \"model\": \"gpt-4o-mini\",\n",
        "      \"messages\": [\n",
        "        {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": [\n",
        "            {\n",
        "              \"type\": \"text\",\n",
        "              \"text\": (\"Are there PPEs in the scene? \"\n",
        "                    \"Are there blue, orange or yellow high visibility vests in the scene? \"\n",
        "                    \"Are there gloves? \"\n",
        "                    \"Are there security boots? \"\n",
        "                    \"Are there hard hats? \"\n",
        "                    \"Count every PPE present in the scene, I'll make you an example:\\n\"\n",
        "                    \"    - People: {number of people}\\n\"\n",
        "                    \"    - Hard_hats: {number of Hard_hats}\\n\"\n",
        "                    \"    - Gloves: {number of Gloves}\\n\"\n",
        "                    \"    - Reflective_vests: {number of Reflective_vests}\\n\"\n",
        "                    \"    - Security_boots: {number of Security_boots}\\n\")\n",
        "            },\n",
        "            {\n",
        "              \"type\": \"image_url\",\n",
        "              \"image_url\": {\n",
        "                \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
        "              }\n",
        "            }\n",
        "          ]\n",
        "        }\n",
        "      ],\n",
        "      \"max_tokens\": 300\n",
        "    }\n",
        "\n",
        "    start = time.time()\n",
        "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
        "    elapsed_time = time.time() - start\n",
        "    # Extracting and printing only the response content\n",
        "    response_json = response.json()\n",
        "    message_content = response_json['choices'][0]['message']['content']\n",
        "    #print(f'file: {filepath}, message_content:\\n {message_content}\\n')\n",
        "\n",
        "    output_file_path = os.path.join(output_dir, f\"{os.path.splitext(file_name)[0]}_output.txt\")\n",
        "    with open(output_file_path, \"w\") as output_file:\n",
        "        output_file.write(f\"Image filename: {file_name}\\n{message_content}\\n\")\n",
        "        output_file.write(f\"Processing time: {elapsed_time:.4f} seconds\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56334d35",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-21T12:39:06.589299Z",
          "iopub.status.busy": "2024-10-21T12:39:06.588860Z",
          "iopub.status.idle": "2024-10-21T12:39:06.622683Z",
          "shell.execute_reply": "2024-10-21T12:39:06.621522Z"
        },
        "papermill": {
          "duration": 0.041144,
          "end_time": "2024-10-21T12:39:06.625155",
          "exception": false,
          "start_time": "2024-10-21T12:39:06.584011",
          "status": "completed"
        },
        "tags": [],
        "id": "56334d35"
      },
      "outputs": [],
      "source": [
        "mapping = [\n",
        "    (\"Worker\", \"People:\"),\n",
        "    (\"Vest\", \"Reflective_vests:\"),\n",
        "    (\"Boots\", \"Security_boots:\"),\n",
        "    (\"Gloves\", \"Gloves:\"),\n",
        "    (\"Hat\", \"Hard_hats:\"),\n",
        "]\n",
        "\n",
        "def calculate_metrics(ground_truth, predictions, mapping):\n",
        "    metrics = {\"TP\": 0, \"FP\": 0, \"FN\": 0}\n",
        "    for class_name, model_key in mapping:\n",
        "        if model_key is None:\n",
        "            pred_count = 0  # No prediction for this class\n",
        "        else:\n",
        "            pred_count = predictions.get(model_key, 0)\n",
        "        true_count = ground_truth.get(class_name, 0)\n",
        "\n",
        "        if pred_count == true_count:\n",
        "            metrics[\"TP\"] += true_count\n",
        "        elif true_count > pred_count:\n",
        "            metrics[\"TP\"] += pred_count\n",
        "            metrics[\"FN\"] += (true_count - pred_count)\n",
        "        else:\n",
        "            metrics[\"TP\"] += true_count\n",
        "            metrics[\"FP\"] += (pred_count - true_count)\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def calculate_metrics_per_DPI(ground_truth, predictions, mapping, dpi):\n",
        "    metrics = {\"TP\": 0, \"FP\": 0, \"FN\": 0}\n",
        "    for class_name, model_key in mapping:\n",
        "        if model_key is None or dpi != model_key.strip(\":\"):\n",
        "            continue  # Skip if the current class does not match the DPI we're focusing on\n",
        "\n",
        "        pred_count = predictions.get(model_key, 0)\n",
        "        true_count = ground_truth.get(class_name, 0)\n",
        "\n",
        "        if pred_count == true_count:\n",
        "            metrics[\"TP\"] += true_count\n",
        "        elif true_count > pred_count:\n",
        "            metrics[\"TP\"] += pred_count\n",
        "            metrics[\"FN\"] += (true_count - pred_count)\n",
        "        else:\n",
        "            metrics[\"TP\"] += true_count\n",
        "            metrics[\"FP\"] += (pred_count - true_count)\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "\n",
        "def read_ground_truth(file_path):\n",
        "    ground_truth = {}\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            elements = line.split()\n",
        "            class_id = int(elements[0])\n",
        "            if class_id == 0:\n",
        "                ground_truth[\"Gloves\"] = ground_truth.get(\"Gloves\", 0) + 1\n",
        "            elif class_id == 1:\n",
        "                ground_truth[\"Hat\"] = ground_truth.get(\"Hat\", 0) + 1\n",
        "            elif class_id == 4:\n",
        "                ground_truth[\"Worker\"] = ground_truth.get(\"Worker\", 0) + 1\n",
        "            elif class_id == 2:\n",
        "                ground_truth[\"Boots\"] = ground_truth.get(\"Boots\", 0) + 1\n",
        "            elif class_id == 3:\n",
        "                ground_truth[\"Vest\"] = ground_truth.get(\"Vest\", 0) + 1\n",
        "    return ground_truth\n",
        "\n",
        "\n",
        "def extract_json_from_file(file_path):\n",
        "    result = {}\n",
        "    found = False\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "        for line in lines:\n",
        "            line = line.lstrip()\n",
        "            if line.startswith('-'):\n",
        "                DPI = ''\n",
        "                found = True\n",
        "                words = line.split()\n",
        "                flag = True\n",
        "                for word in words:\n",
        "                    if not flag:\n",
        "                        break\n",
        "                    if word == 'People:' or word == 'Gloves:' or word == 'Security_boots:' or word == 'Reflective_vests:' or word == 'Hard_hats:':\n",
        "                        DPI = word\n",
        "                        found = True\n",
        "                    if word.isdigit():\n",
        "                        result[DPI] = result.get(DPI, 0) + int(word)\n",
        "                        flag = False\n",
        "                    if word == 'Yes':\n",
        "                        result[DPI] = result.get(DPI, 0) + 1\n",
        "                        flag = False\n",
        "                    if word == 'No' or word.lower() == 'not':\n",
        "                        result[DPI] = result.get(DPI, 0)\n",
        "                        flag = False\n",
        "        if not found:\n",
        "            result['People:'] = result.get('People:', 0)\n",
        "            result['Gloves:'] = result.get('Gloves:', 0)\n",
        "            result['Security_boots:'] = result.get('Security_boots:', 0)\n",
        "            result['Reflective_vests:'] = result.get('Reflective_vests:', 0)\n",
        "            result['Hard_hats:'] = result.get('Hard_hats:', 0)\n",
        "    #print(f'result: {result}')\n",
        "    return result\n",
        "\n",
        "\n",
        "\n",
        "def filter_greater_than_zero(data):\n",
        "    return {key: value for key, value in data.items() if value > 0}\n",
        "\n",
        "def calculate_classification_metrics(metrics):\n",
        "    TP = metrics[\"TP\"]\n",
        "    FP = metrics[\"FP\"]\n",
        "    FN = metrics[\"FN\"]\n",
        "\n",
        "    accuracy = TP / (TP + FP + FN) if (TP + FP + FN) > 0 else 0\n",
        "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1_score\": f1_score\n",
        "    }\n",
        "\n",
        "def save_metrics_to_file(metrics, output_folder, file_name):\n",
        "    output_path = os.path.join(output_folder, f\"{file_name}_metrics.json\")\n",
        "    with open(output_path, 'w') as f:\n",
        "        json.dump(metrics, f, indent=4)\n",
        "    #print(f\"Metrics saved to {output_path}\")\n",
        "\n",
        "def calculate_final_metrics(all_metrics):\n",
        "    total_accuracy = 0\n",
        "    total_precision = 0\n",
        "    total_recall = 0\n",
        "    total_f1_score = 0\n",
        "    n = len(all_metrics)\n",
        "    # Somma i valori delle metriche per ciascun file\n",
        "    for metric in all_metrics:\n",
        "        total_accuracy += metric['accuracy']\n",
        "        total_precision += metric['precision']\n",
        "        total_recall += metric['recall']\n",
        "        total_f1_score += metric['f1_score']\n",
        "\n",
        "\n",
        "    # Calcola la media per ciascuna metrica\n",
        "    final_metrics = {\n",
        "        \"final_accuracy\": total_accuracy / n,\n",
        "        \"final_precision\": total_precision / n,\n",
        "        \"final_recall\": total_recall / n,\n",
        "        \"final_f1_score\": total_f1_score / n\n",
        "    }\n",
        "    return final_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "800e0101",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-21T12:39:06.632477Z",
          "iopub.status.busy": "2024-10-21T12:39:06.632059Z",
          "iopub.status.idle": "2024-10-21T12:39:10.991966Z",
          "shell.execute_reply": "2024-10-21T12:39:10.990702Z"
        },
        "papermill": {
          "duration": 4.366947,
          "end_time": "2024-10-21T12:39:10.994994",
          "exception": false,
          "start_time": "2024-10-21T12:39:06.628047",
          "status": "completed"
        },
        "tags": [],
        "id": "800e0101",
        "outputId": "665c39a3-8223-41b1-855c-a939b73e1847"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "final metric: {'final_accuracy': 0.8032928061345777, 'final_precision': 0.8471224237018296, 'final_recall': 0.9166947217351059, 'final_f1_score': 0.8661942533951766}\n",
            "final metric People: {'final_accuracy': 0.8888412484140337, 'final_precision': 0.9084613083821947, 'final_recall': 0.9455698134495604, 'final_f1_score': 0.9160310608506423}\n",
            "final metric Hard_hats: {'final_accuracy': 0.8067635034200803, 'final_precision': 0.8287334237492466, 'final_recall': 0.8641060290379222, 'final_f1_score': 0.8357906732533263}\n",
            "final metric Gloves: {'final_accuracy': 0.11334388185654007, 'final_precision': 0.14477848101265822, 'final_recall': 0.12362869198312235, 'final_f1_score': 0.12758552980071966}\n",
            "final metric Reflective_vests: {'final_accuracy': 0.3914494173196704, 'final_precision': 0.39998618645770545, 'final_recall': 0.41234930681133203, 'final_f1_score': 0.40191314228916597}\n",
            "final metric Security_boots: {'final_accuracy': 0.03385664967943449, 'final_precision': 0.04641350210970464, 'final_recall': 0.03807605896213491, 'final_f1_score': 0.03989957873202377}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "ground_truth_folder = \"/kaggle/input/ppe-detection-gpt-version/PPE_Detection 2.v1i.yolov8-obb/test/labels\"\n",
        "predictions_folder = \"/kaggle/input/output-gpt-4o-mini/test_set\"\n",
        "output_folder = \"/kaggle/working/metrics_output\"\n",
        "output_folder_final = \"/kaggle/working/metrics_output_final\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "os.makedirs(output_folder_final, exist_ok=True)\n",
        "DPI = ['People', 'Hard_hats', 'Gloves', 'Reflective_vests', 'Security_boots']\n",
        "\n",
        "gt_files = sorted(os.listdir(ground_truth_folder))\n",
        "pred_files = sorted(os.listdir(predictions_folder))\n",
        "all_metrics = []\n",
        "for gt_file, pred_file in zip(gt_files, pred_files):\n",
        "    base_filename = os.path.splitext(pred_file)[0]\n",
        "    gt_results = read_ground_truth(os.path.join(ground_truth_folder,gt_file))\n",
        "    data = extract_json_from_file(os.path.join(predictions_folder, pred_file))\n",
        "    prediction_results = filter_greater_than_zero(data)\n",
        "    metrics = calculate_metrics(gt_results, prediction_results, mapping)\n",
        "    final_metrics_per_image = calculate_classification_metrics(metrics)\n",
        "    all_metrics.append(final_metrics_per_image)\n",
        "final = calculate_final_metrics(all_metrics)\n",
        "print(f'final metric: {final}')\n",
        "save_metrics_to_file(final, output_folder_final, \"final\")\n",
        "\n",
        "#########################################################################################################\n",
        "final_ppes_metrics = {\n",
        "    'People': {\n",
        "        'Accuracy': 0,\n",
        "        'Precision': 0,\n",
        "        'Recall': 0,\n",
        "        'F1-Score': 0\n",
        "    },\n",
        "    'Hard_hats': {\n",
        "        'Accuracy': 0,\n",
        "        'Precision': 0,\n",
        "        'Recall': 0,\n",
        "        'F1-Score': 0\n",
        "    },\n",
        "    'Gloves': {\n",
        "        'Accuracy': 0,\n",
        "        'Precision': 0,\n",
        "        'Recall': 0,\n",
        "        'F1-Score': 0\n",
        "    },\n",
        "    'Reflective_vests': {\n",
        "        'Accuracy': 0,\n",
        "        'Precision': 0,\n",
        "        'Recall': 0,\n",
        "        'F1-Score': 0\n",
        "    },\n",
        "    'Security_boots': {\n",
        "        'Accuracy': 0,\n",
        "        'Precision': 0,\n",
        "        'Recall': 0,\n",
        "        'F1-Score': 0\n",
        "    },\n",
        "}\n",
        "# Second part of the code to calculate per DPI metrics\n",
        "gt_files = sorted(os.listdir(ground_truth_folder))\n",
        "pred_files = sorted(os.listdir(predictions_folder))\n",
        "\n",
        "# Iterate over each DPI type\n",
        "for dpi in DPI:\n",
        "    all_metrics_per_dpi = []  # Initialize a list to collect metrics for each DPI\n",
        "    for gt_file, pred_file in zip(gt_files, pred_files):\n",
        "        base_filename = os.path.splitext(pred_file)[0]\n",
        "\n",
        "        # Read ground truth and predictions for the current image\n",
        "        gt_results = read_ground_truth(os.path.join(ground_truth_folder, gt_file))\n",
        "        data = extract_json_from_file(os.path.join(predictions_folder, pred_file))\n",
        "\n",
        "        # Filter predictions with counts greater than zero\n",
        "        prediction_results = filter_greater_than_zero(data)\n",
        "\n",
        "        # Calculate per-image metrics for the current DPI\n",
        "        metrics = calculate_metrics_per_DPI(gt_results, prediction_results, mapping, dpi)\n",
        "        final_metrics_per_image = calculate_classification_metrics(metrics)\n",
        "        all_metrics_per_dpi.append(final_metrics_per_image)\n",
        "\n",
        "    # Calculate final averaged metrics for the current DPI\n",
        "    final_dpi_metrics = calculate_final_metrics(all_metrics_per_dpi)\n",
        "    print(f'final metric {dpi}: {final_dpi_metrics}')\n",
        "\n",
        "    # Update the final PPE metrics dictionary with calculated values\n",
        "    final_ppes_metrics[dpi]['Accuracy'] = final_dpi_metrics['final_accuracy']\n",
        "    final_ppes_metrics[dpi]['Precision'] = final_dpi_metrics['final_precision']\n",
        "    final_ppes_metrics[dpi]['Recall'] = final_dpi_metrics['final_recall']\n",
        "    final_ppes_metrics[dpi]['F1-Score'] = final_dpi_metrics['final_f1_score']\n",
        "\n",
        "# Save final PPEs metrics to file\n",
        "output_file_path = os.path.join(output_folder_final, \"final_ppes_metrics.json\")\n",
        "with open(output_file_path, 'w') as f:\n",
        "    json.dump(final_ppes_metrics, f, indent=4)"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 5852665,
          "sourceId": 9594799,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 5853129,
          "sourceId": 9595454,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30786,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 1393.108165,
      "end_time": "2024-10-21T12:39:11.419035",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2024-10-21T12:15:58.310870",
      "version": "2.6.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}