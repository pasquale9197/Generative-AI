{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b7a8737",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2024-10-21T12:40:42.319360Z",
          "iopub.status.busy": "2024-10-21T12:40:42.318943Z",
          "iopub.status.idle": "2024-10-21T12:59:10.841021Z",
          "shell.execute_reply": "2024-10-21T12:59:10.839935Z"
        },
        "papermill": {
          "duration": 1108.529207,
          "end_time": "2024-10-21T12:59:10.843710",
          "exception": false,
          "start_time": "2024-10-21T12:40:42.314503",
          "status": "completed"
        },
        "tags": [],
        "id": "8b7a8737"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "import requests\n",
        "import os\n",
        "import time\n",
        "\n",
        "# OpenAI API Key\n",
        "api_key = \"*************\"\n",
        "\n",
        "# Function to encode the image\n",
        "def encode_image(image_path):\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
        "\n",
        "# Path to your image\n",
        "input_dir = \"/kaggle/input/gpt-dataset-final/PPE_Detection 2.v1i.yolov8-obb/test/images\"\n",
        "output_dir = \"/kaggle/working/test_set\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "for file_name in os.listdir(input_dir):\n",
        "    filepath = os.path.join(input_dir, file_name)\n",
        "    base64_image = encode_image(filepath)\n",
        "    headers = {\n",
        "      \"Content-Type\": \"application/json\",\n",
        "      \"Authorization\": f\"Bearer {api_key}\"\n",
        "    }\n",
        "    payload = {\n",
        "      \"model\": \"gpt-4o\",\n",
        "      \"messages\": [\n",
        "        {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": [\n",
        "            {\n",
        "              \"type\": \"text\",\n",
        "              \"text\": (\"Are there PPEs in the scene? \"\n",
        "                    \"Are there blue, orange or yellow high visibility vests in the scene? \"\n",
        "                    \"Are there gloves? \"\n",
        "                    \"Are there security boots? \"\n",
        "                    \"Are there hard hats? \"\n",
        "                    \"Count every PPE present in the scene, I'll make you an example:\\n\"\n",
        "                    \"    - People: {number of people}\\n\"\n",
        "                    \"    - Hard_hats: {number of Hard_hats}\\n\"\n",
        "                    \"    - Gloves: {number of Gloves}\\n\"\n",
        "                    \"    - Reflective_vests: {number of Reflective_vests}\\n\"\n",
        "                    \"    - Security_boots: {number of Security_boots}\\n\")\n",
        "            },\n",
        "            {\n",
        "              \"type\": \"image_url\",\n",
        "              \"image_url\": {\n",
        "                \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
        "              }\n",
        "            }\n",
        "          ]\n",
        "        }\n",
        "      ],\n",
        "      \"max_tokens\": 500\n",
        "    }\n",
        "    start = time.time()\n",
        "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
        "    elapsed_time = time.time() - start\n",
        "    # Extracting and printing only the response content\n",
        "    response_json = response.json()\n",
        "    message_content = response_json['choices'][0]['message']['content']\n",
        "    #print(f'file: {filepath}, message_content:\\n {message_content}\\n')\n",
        "\n",
        "    output_file_path = os.path.join(output_dir, f\"{os.path.splitext(file_name)[0]}_output.txt\")\n",
        "    with open(output_file_path, \"w\") as output_file:\n",
        "        output_file.write(f\"Image filename: {file_name}\\n{message_content}\\n\")\n",
        "        output_file.write(f\"Processing time: {elapsed_time:.4f} seconds\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59f1a3bb",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-21T12:59:10.850942Z",
          "iopub.status.busy": "2024-10-21T12:59:10.850342Z",
          "iopub.status.idle": "2024-10-21T12:59:10.876625Z",
          "shell.execute_reply": "2024-10-21T12:59:10.875637Z"
        },
        "papermill": {
          "duration": 0.032698,
          "end_time": "2024-10-21T12:59:10.879128",
          "exception": false,
          "start_time": "2024-10-21T12:59:10.846430",
          "status": "completed"
        },
        "tags": [],
        "id": "59f1a3bb"
      },
      "outputs": [],
      "source": [
        "mapping = [\n",
        "    (\"Worker\", \"People:\"),\n",
        "    (\"Vest\", \"Reflective_vests:\"),\n",
        "    (\"Boots\", \"Security_boots:\"),\n",
        "    (\"Gloves\", \"Gloves:\"),\n",
        "    (\"Hat\", \"Hard_hats:\"),\n",
        "]\n",
        "\n",
        "def calculate_metrics(ground_truth, predictions, mapping):\n",
        "    metrics = {\"TP\": 0, \"FP\": 0, \"FN\": 0}\n",
        "    for class_name, model_key in mapping:\n",
        "        if model_key is None:\n",
        "            pred_count = 0  # No prediction for this class\n",
        "        else:\n",
        "            pred_count = predictions.get(model_key, 0)\n",
        "        true_count = ground_truth.get(class_name, 0)\n",
        "\n",
        "        if pred_count == true_count:\n",
        "            metrics[\"TP\"] += true_count\n",
        "        elif true_count > pred_count:\n",
        "            metrics[\"TP\"] += pred_count\n",
        "            metrics[\"FN\"] += (true_count - pred_count)\n",
        "        else:\n",
        "            metrics[\"TP\"] += true_count\n",
        "            metrics[\"FP\"] += (pred_count - true_count)\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def calculate_metrics_per_DPI(ground_truth, predictions, mapping, dpi):\n",
        "    metrics = {\"TP\": 0, \"FP\": 0, \"FN\": 0}\n",
        "    for class_name, model_key in mapping:\n",
        "        if model_key is None or dpi != model_key.strip(\":\"):\n",
        "            continue  # Skip if the current class does not match the DPI we're focusing on\n",
        "\n",
        "        pred_count = predictions.get(model_key, 0)\n",
        "        true_count = ground_truth.get(class_name, 0)\n",
        "\n",
        "        if pred_count == true_count:\n",
        "            metrics[\"TP\"] += true_count\n",
        "        elif true_count > pred_count:\n",
        "            metrics[\"TP\"] += pred_count\n",
        "            metrics[\"FN\"] += (true_count - pred_count)\n",
        "        else:\n",
        "            metrics[\"TP\"] += true_count\n",
        "            metrics[\"FP\"] += (pred_count - true_count)\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "\n",
        "def read_ground_truth(file_path):\n",
        "    ground_truth = {}\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            elements = line.split()\n",
        "            class_id = int(elements[0])\n",
        "            if class_id == 0:\n",
        "                ground_truth[\"Gloves\"] = ground_truth.get(\"Gloves\", 0) + 1\n",
        "            elif class_id == 1:\n",
        "                ground_truth[\"Hat\"] = ground_truth.get(\"Hat\", 0) + 1\n",
        "            elif class_id == 4:\n",
        "                ground_truth[\"Worker\"] = ground_truth.get(\"Worker\", 0) + 1\n",
        "            elif class_id == 2:\n",
        "                ground_truth[\"Boots\"] = ground_truth.get(\"Boots\", 0) + 1\n",
        "            elif class_id == 3:\n",
        "                ground_truth[\"Vest\"] = ground_truth.get(\"Vest\", 0) + 1\n",
        "    return ground_truth\n",
        "\n",
        "\n",
        "def extract_json_from_file(file_path):\n",
        "    result = {}\n",
        "    found = False\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "        for line in lines:\n",
        "            line = line.lstrip()\n",
        "            if line.startswith('-'):\n",
        "                DPI = ''\n",
        "                found = True\n",
        "                words = line.split()\n",
        "                flag = True\n",
        "                for word in words:\n",
        "                    if not flag:\n",
        "                        break\n",
        "                    if word == 'People:' or word == 'Gloves:' or word == 'Security_boots:' or word == 'Reflective_vests:' or word == 'Hard_hats:':\n",
        "                        DPI = word\n",
        "                        found = True\n",
        "                    if word.isdigit():\n",
        "                        result[DPI] = result.get(DPI, 0) + int(word)\n",
        "                        flag = False\n",
        "                    if word == 'Yes':\n",
        "                        result[DPI] = result.get(DPI, 0) + 1\n",
        "                        flag = False\n",
        "                    if word == 'No' or word.lower() == 'not':\n",
        "                        result[DPI] = result.get(DPI, 0)\n",
        "                        flag = False\n",
        "        if not found:\n",
        "            result['People:'] = result.get('People:', 0)\n",
        "            result['Gloves:'] = result.get('Gloves:', 0)\n",
        "            result['Security_boots:'] = result.get('Security_boots:', 0)\n",
        "            result['Reflective_vests:'] = result.get('Reflective_vests:', 0)\n",
        "            result['Hard_hats:'] = result.get('Hard_hats:', 0)\n",
        "    #print(f'result: {result}')\n",
        "    return result\n",
        "\n",
        "\n",
        "\n",
        "def filter_greater_than_zero(data):\n",
        "    return {key: value for key, value in data.items() if value > 0}\n",
        "\n",
        "def calculate_classification_metrics(metrics):\n",
        "    TP = metrics[\"TP\"]\n",
        "    FP = metrics[\"FP\"]\n",
        "    FN = metrics[\"FN\"]\n",
        "\n",
        "    accuracy = TP / (TP + FP + FN) if (TP + FP + FN) > 0 else 0\n",
        "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1_score\": f1_score\n",
        "    }\n",
        "\n",
        "def save_metrics_to_file(metrics, output_folder, file_name):\n",
        "    output_path = os.path.join(output_folder, f\"{file_name}_metrics.json\")\n",
        "    with open(output_path, 'w') as f:\n",
        "        json.dump(metrics, f, indent=4)\n",
        "    #print(f\"Metrics saved to {output_path}\")\n",
        "\n",
        "def calculate_final_metrics(all_metrics):\n",
        "    total_accuracy = 0\n",
        "    total_precision = 0\n",
        "    total_recall = 0\n",
        "    total_f1_score = 0\n",
        "    n = len(all_metrics)\n",
        "    # Somma i valori delle metriche per ciascun file\n",
        "    for metric in all_metrics:\n",
        "        total_accuracy += metric['accuracy']\n",
        "        total_precision += metric['precision']\n",
        "        total_recall += metric['recall']\n",
        "        total_f1_score += metric['f1_score']\n",
        "\n",
        "\n",
        "    # Calcola la media per ciascuna metrica\n",
        "    final_metrics = {\n",
        "        \"final_accuracy\": total_accuracy / n,\n",
        "        \"final_precision\": total_precision / n,\n",
        "        \"final_recall\": total_recall / n,\n",
        "        \"final_f1_score\": total_f1_score / n\n",
        "    }\n",
        "    return final_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61bf0f96",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-21T12:59:10.885705Z",
          "iopub.status.busy": "2024-10-21T12:59:10.884910Z",
          "iopub.status.idle": "2024-10-21T12:59:12.876925Z",
          "shell.execute_reply": "2024-10-21T12:59:12.875862Z"
        },
        "papermill": {
          "duration": 1.997811,
          "end_time": "2024-10-21T12:59:12.879261",
          "exception": false,
          "start_time": "2024-10-21T12:59:10.881450",
          "status": "completed"
        },
        "tags": [],
        "id": "61bf0f96",
        "outputId": "3dc2b40a-117d-4122-f5fd-abbd3ee7a302"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "final metric: {'final_accuracy': 0.835035749681239, 'final_precision': 0.8877078918102266, 'final_recall': 0.9345083780727107, 'final_f1_score': 0.8973454519978932}\n",
            "final metric People: {'final_accuracy': 0.9135960075437926, 'final_precision': 0.9370922150985443, 'final_recall': 0.9670101215591723, 'final_f1_score': 0.9415358220425202}\n",
            "final metric Hard_hats: {'final_accuracy': 0.8458324498225432, 'final_precision': 0.8760295084662173, 'final_recall': 0.8811953464196172, 'final_f1_score': 0.8710365333383813}\n",
            "final metric Gloves: {'final_accuracy': 0.1248945147679325, 'final_precision': 0.177373417721519, 'final_recall': 0.13422995780590719, 'final_f1_score': 0.14479720349973507}\n",
            "final metric Reflective_vests: {'final_accuracy': 0.3928941311852704, 'final_precision': 0.3983341552961806, 'final_recall': 0.4122814948764316, 'final_f1_score': 0.4014231910639207}\n",
            "final metric Security_boots: {'final_accuracy': 0.032775426050742504, 'final_precision': 0.050237341772151896, 'final_recall': 0.0395001095950463, 'final_f1_score': 0.04011488733340832}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "ground_truth_folder = \"/kaggle/input/gpt-dataset-final/PPE_Detection 2.v1i.yolov8-obb/test/labels\"\n",
        "predictions_folder = \"/kaggle/working/test_set\"\n",
        "output_folder = \"/kaggle/working/metrics_output\"\n",
        "output_folder_final = \"/kaggle/working/metrics_output_final\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "os.makedirs(output_folder_final, exist_ok=True)\n",
        "DPI = ['People', 'Hard_hats', 'Gloves', 'Reflective_vests', 'Security_boots']\n",
        "\n",
        "gt_files = sorted(os.listdir(ground_truth_folder))\n",
        "pred_files = sorted(os.listdir(predictions_folder))\n",
        "all_metrics = []\n",
        "for gt_file, pred_file in zip(gt_files, pred_files):\n",
        "    base_filename = os.path.splitext(pred_file)[0]\n",
        "    gt_results = read_ground_truth(os.path.join(ground_truth_folder,gt_file))\n",
        "    data = extract_json_from_file(os.path.join(predictions_folder, pred_file))\n",
        "    prediction_results = filter_greater_than_zero(data)\n",
        "    metrics = calculate_metrics(gt_results, prediction_results, mapping)\n",
        "    final_metrics_per_image = calculate_classification_metrics(metrics)\n",
        "    all_metrics.append(final_metrics_per_image)\n",
        "final = calculate_final_metrics(all_metrics)\n",
        "print(f'final metric: {final}')\n",
        "save_metrics_to_file(final, output_folder_final, \"final\")\n",
        "\n",
        "#########################################################################################################\n",
        "final_ppes_metrics = {\n",
        "    'People': {\n",
        "        'Accuracy': 0,\n",
        "        'Precision': 0,\n",
        "        'Recall': 0,\n",
        "        'F1-Score': 0\n",
        "    },\n",
        "    'Hard_hats': {\n",
        "        'Accuracy': 0,\n",
        "        'Precision': 0,\n",
        "        'Recall': 0,\n",
        "        'F1-Score': 0\n",
        "    },\n",
        "    'Gloves': {\n",
        "        'Accuracy': 0,\n",
        "        'Precision': 0,\n",
        "        'Recall': 0,\n",
        "        'F1-Score': 0\n",
        "    },\n",
        "    'Reflective_vests': {\n",
        "        'Accuracy': 0,\n",
        "        'Precision': 0,\n",
        "        'Recall': 0,\n",
        "        'F1-Score': 0\n",
        "    },\n",
        "    'Security_boots': {\n",
        "        'Accuracy': 0,\n",
        "        'Precision': 0,\n",
        "        'Recall': 0,\n",
        "        'F1-Score': 0\n",
        "    },\n",
        "}\n",
        "# Second part of the code to calculate per DPI metrics\n",
        "gt_files = sorted(os.listdir(ground_truth_folder))\n",
        "pred_files = sorted(os.listdir(predictions_folder))\n",
        "\n",
        "# Iterate over each DPI type\n",
        "for dpi in DPI:\n",
        "    all_metrics_per_dpi = []  # Initialize a list to collect metrics for each DPI\n",
        "    for gt_file, pred_file in zip(gt_files, pred_files):\n",
        "        base_filename = os.path.splitext(pred_file)[0]\n",
        "\n",
        "        # Read ground truth and predictions for the current image\n",
        "        gt_results = read_ground_truth(os.path.join(ground_truth_folder, gt_file))\n",
        "        data = extract_json_from_file(os.path.join(predictions_folder, pred_file))\n",
        "\n",
        "        # Filter predictions with counts greater than zero\n",
        "        prediction_results = filter_greater_than_zero(data)\n",
        "\n",
        "        # Calculate per-image metrics for the current DPI\n",
        "        metrics = calculate_metrics_per_DPI(gt_results, prediction_results, mapping, dpi)\n",
        "        final_metrics_per_image = calculate_classification_metrics(metrics)\n",
        "        all_metrics_per_dpi.append(final_metrics_per_image)\n",
        "\n",
        "    # Calculate final averaged metrics for the current DPI\n",
        "    final_dpi_metrics = calculate_final_metrics(all_metrics_per_dpi)\n",
        "    print(f'final metric {dpi}: {final_dpi_metrics}')\n",
        "\n",
        "    # Update the final PPE metrics dictionary with calculated values\n",
        "    final_ppes_metrics[dpi]['Accuracy'] = final_dpi_metrics['final_accuracy']\n",
        "    final_ppes_metrics[dpi]['Precision'] = final_dpi_metrics['final_precision']\n",
        "    final_ppes_metrics[dpi]['Recall'] = final_dpi_metrics['final_recall']\n",
        "    final_ppes_metrics[dpi]['F1-Score'] = final_dpi_metrics['final_f1_score']\n",
        "\n",
        "# Save final PPEs metrics to file\n",
        "output_file_path = os.path.join(output_folder_final, \"final_ppes_metrics.json\")\n",
        "with open(output_file_path, 'w') as f:\n",
        "    json.dump(final_ppes_metrics, f, indent=4)"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 5853442,
          "sourceId": 9595878,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30786,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 1113.596913,
      "end_time": "2024-10-21T12:59:13.302809",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2024-10-21T12:40:39.705896",
      "version": "2.6.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}